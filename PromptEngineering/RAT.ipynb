{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enviroment setup"
      ],
      "metadata": {
        "id": "BSHL95IZQzfe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VEkA47VDQmRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db1b6e1-513b-4f75-9712-f35045f9fd85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m812.8/812.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-core langchain-community langchain-google-genai \\\n",
        "                                google-generativeai \\\n",
        "                                tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import neccessary libraries"
      ],
      "metadata": {
        "id": "1bOVtfe1Q3MP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "from multiprocessing import Process, Queue\n",
        "\n",
        "from difflib import unified_diff\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "import google.generativeai as genai\n",
        "\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.tools import Tool\n",
        "\n",
        "from langchain_community.utilities import GoogleSearchAPIWrapper\n",
        "from langchain_community.document_transformers import Html2TextTransformer\n",
        "from langchain_community.document_loaders import AsyncHtmlLoader"
      ],
      "metadata": {
        "id": "7xntRLnWQoMj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = userdata.get('GOOGLE_CSE_ID')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "genai.configure(api_key=GEMINI_API_KEY)"
      ],
      "metadata": {
        "id": "fjVRofDbQoPB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "pwDfcyecQ9L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_search(query:str=\"\", k:int=1): # get the top-k resources with google\n",
        "    search = GoogleSearchAPIWrapper(k=k)\n",
        "    def search_results(query):\n",
        "        return search.results(query, k)\n",
        "    tool = Tool(\n",
        "        name=\"Google Search Snippets\",\n",
        "        description=\"Search Google for recent results.\",\n",
        "        func=search_results,\n",
        "    )\n",
        "    ref_text = tool.run(query)\n",
        "    if 'Result' not in ref_text[0].keys():\n",
        "        return ref_text\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "u0eXSM3JQoRm"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test GoogleSearchAPIWrapper\n",
        "\n",
        "First, you need to set up the proper API keys and environment variables. To set it up, create the GOOGLE_API_KEY in the Google Cloud credential console (https://console.cloud.google.com/apis/credentials) and a GOOGLE_CSE_ID using the Programmable Search Engine (https://programmablesearchengine.google.com/controlpanel/create).\n",
        "\n",
        "More details:\n",
        "+ https://python.langchain.com/docs/integrations/tools/google_search\n",
        "\n",
        "I recommend you guys read this to setup GoogleSearchAPIWrapper:\n",
        "+ https://stackoverflow.com/questions/37083058/programmatically-searching-google-in-python-using-custom-search"
      ],
      "metadata": {
        "id": "lHc9tK_ySmMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"how can i have a girlfriend\"\n",
        "get_search(query=query, k=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_YHBW_lSpf4",
        "outputId": "4973dc75-e8d3-45d0-8257-b2003d84ad3d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'What is the best way to find a girlfriend? : r/NoStupidQuestions',\n",
              "  'link': 'https://www.reddit.com/r/NoStupidQuestions/comments/psbap8/what_is_the_best_way_to_find_a_girlfriend/',\n",
              "  'snippet': 'Sep 21, 2021 ... Look for some sort of clubs, singles groups, meet ups, any kind of casual gatherings where you can get more comfortable in social situations.'},\n",
              " {'title': 'What is the easiest and fastest way to get a girlfriend? How I can get ...',\n",
              "  'link': 'https://www.quora.com/What-is-the-easiest-and-fastest-way-to-get-a-girlfriend-How-I-can-get-one-I-am-23-years-old-and-never-had-a-girlfriend-I-am-shy-around-women-I-want-to-make-it-happen-Please-give-me-good-advice',\n",
              "  'snippet': 'May 13, 2020 ... Practice talking to girls. Get comfortable with it. Perhaps even make female friends. That could lead to something. · Practice talking to girls\\xa0...'},\n",
              " {'title': '[serious] What is it like to have a girlfriend? : r/AskReddit',\n",
              "  'link': 'https://www.reddit.com/r/AskReddit/comments/3i9bjp/serious_what_is_it_like_to_have_a_girlfriend/',\n",
              "  'snippet': \"Aug 24, 2015 ... It's kind of like owning a dog. Fantastic for having someone around that cares about you all the time and is enthusiastic about doing things\\xa0...\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_page_content(link:str):\n",
        "    loader = AsyncHtmlLoader([link])\n",
        "    docs = loader.load()\n",
        "    html2text = Html2TextTransformer()\n",
        "    docs_transformed = html2text.transform_documents(docs)\n",
        "    if len(docs_transformed) > 0:\n",
        "        return docs_transformed[0].page_content\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "Ze1YZHQXQoT9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "def chunk_text_by_sentence(text, chunk_size=2048):\n",
        "    \"\"\"Chunk the $text into sentences with less than 2k tokens.\"\"\"\n",
        "    sentences = text.split('. ')\n",
        "    chunked_text = []\n",
        "    curr_chunk = []\n",
        "    # Gradually add text fragments, ensuring that each paragraph has less than 2k tokens\n",
        "    for sentence in sentences:\n",
        "        if num_tokens_from_string(\". \".join(curr_chunk)) + num_tokens_from_string(sentence) + 2 <= chunk_size:\n",
        "            curr_chunk.append(sentence)\n",
        "        else:\n",
        "            chunked_text.append(\". \".join(curr_chunk))\n",
        "            curr_chunk = [sentence]\n",
        "    # Add last fragment\n",
        "    if curr_chunk:\n",
        "        chunked_text.append(\". \".join(curr_chunk))\n",
        "    return chunked_text[0]\n",
        "\n",
        "def chunk_text_front(text, chunk_size = 2048):\n",
        "    '''\n",
        "    get the first `trunk_size` token of text\n",
        "    '''\n",
        "    chunked_text = \"\"\n",
        "    tokens = num_tokens_from_string(text)\n",
        "    if tokens < chunk_size:\n",
        "        return text\n",
        "    else:\n",
        "        ratio = float(chunk_size) / tokens\n",
        "        char_num = int(len(text) * ratio)\n",
        "        return text[:char_num]\n",
        "\n",
        "def chunk_texts(text, chunk_size = 2048):\n",
        "    '''\n",
        "    trunk the text into n parts, return a list of text\n",
        "    [text, text, text]\n",
        "    '''\n",
        "    tokens = num_tokens_from_string(text)\n",
        "    if tokens < chunk_size:\n",
        "        return [text]\n",
        "    else:\n",
        "        texts = []\n",
        "        n = int(tokens/chunk_size) + 1\n",
        "        # Calculate the length of each section\n",
        "        part_length = len(text) // n\n",
        "        # If not divisible, the last part will contain extra characters\n",
        "        extra = len(text) % n\n",
        "        parts = []\n",
        "        start = 0\n",
        "\n",
        "        for i in range(n):\n",
        "            # For the first extra parts, one more character is allocated for each part\n",
        "            end = start + part_length + (1 if i < extra else 0)\n",
        "            parts.append(text[start:end])\n",
        "            start = end\n",
        "        return parts\n"
      ],
      "metadata": {
        "id": "8S1kbAQvQoWs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "gOnjYzuNQ-oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_system_prompt = f'''\n",
        "You are Gemini, is a large language model created by Google AI trained on a massive dataset of text and code. As a powerful tool, you can help with various tasks such as writing, research, problem-solving, and translation. As a machine learning model, you are constantly learning and improving.\n",
        "Knowledge cutoff: 2024-02\n",
        "Current date: {datetime.now().strftime('%Y-%m-%d')}\n",
        "'''\n",
        "\n",
        "def get_draft(question):\n",
        "  # Getting the draft answer\n",
        "  draft_prompt = '''\n",
        "  IMPORTANT:\n",
        "  Try to answer this question/instruction with step-by-step thoughts and make the answer more structural.\n",
        "  Use `\\n\\n` to split the answer into several paragraphs.\n",
        "  Just respond to the instruction directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
        "  '''\n",
        "  draft_model = genai.GenerativeModel('gemini-pro')\n",
        "  messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"parts\": [f\"{gemini_system_prompt}\\n{question}\" + draft_prompt]\n",
        "            }\n",
        "        ]\n",
        "  response = draft_model.generate_content(messages,\n",
        "                                    generation_config=genai.types.GenerationConfig(\n",
        "                                        temperature=1.0,\n",
        "                                    ))\n",
        "  try:\n",
        "    result = response.text\n",
        "    return result\n",
        "  except Exception as e:\n",
        "    print(f'{type(e).__name__}: {e}')\n",
        "    return None"
      ],
      "metadata": {
        "id": "ukUJjWudQoZe"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is an apple?\"\n",
        "sample_ans = get_draft(question)\n",
        "sample_ans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "j7iN2iUOl0Ey",
        "outputId": "9dcfbf1b-aaec-4bbf-be19-73bdacf81e29"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. **Identify the task:** The task is to define \"apple\".\\n2. **Retrieve relevant knowledge:** As a large language model, I have comprehensive knowledge about apples.\\n3. **Organize the knowledge:**\\n   - Apples are fruits.\\n   - They typically have a round shape and red skin, but can vary in color and shape.\\n   - They are crunchy and juicy, with a sweet or tart flavor.\\n   - They have a core with seeds.\\n   - They are a good source of fiber and vitamins.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_draft(draft, split_char = '\\n\\n'):\n",
        "  # Split the draft into multiple paragraphs\n",
        "  # split_char: '\\n\\n'\n",
        "  draft_paragraphs = draft.split(split_char)\n",
        "  # print(f\"The draft answer has {len(draft_paragraphs)}\")\n",
        "  return draft_paragraphs"
      ],
      "metadata": {
        "id": "zKfsEusdj1Od"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query(question, answer):\n",
        "  query_prompt = '''\n",
        "I want to verify the content correctness of the given question, especially the last sentences.\n",
        "Please summarize the content with the corresponding question.\n",
        "This summarization will be used as a query to search with Bing search engine.\n",
        "The query should be short but need to be specific to promise Bing can find related knowledge or pages.\n",
        "You can also use search syntax to make the query short and clear enough for the search engine to find relevant language data.\n",
        "Try to make the query as relevant as possible to the last few sentences in the content.\n",
        "**IMPORTANT**\n",
        "Just output the query directly. DO NOT add additional explanations or introducement in the answer unless you are asked to.\n",
        "'''\n",
        "  query_model = genai.GenerativeModel('gemini-pro')\n",
        "  messages=[\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"parts\": f\"{gemini_system_prompt}\\n##Question: {question}\\n\\n##Content: {answer}\\n\\n##Instruction: {query_prompt}\"\n",
        "      }\n",
        "  ]\n",
        "  query = query_model.generate_content(messages,\n",
        "                                    generation_config=genai.types.GenerationConfig(\n",
        "                                        temperature=1.0,\n",
        "                                        top_k=3,\n",
        "                                        top_p=0.8\n",
        "                                    ))\n",
        "  try:\n",
        "    result = query.text\n",
        "    return result\n",
        "  except Exception as e:\n",
        "    print(f'{type(e).__name__}: {e}')\n",
        "    return None"
      ],
      "metadata": {
        "id": "tap6x1jyQocC"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_query(question, sample_ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9NowmM_Fo9i4",
        "outputId": "f33d05e6-c0e1-45bf-b402-025b6a678bb8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Apple nutritional value'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_content(query):\n",
        "  res = get_search(query, 1)\n",
        "  if not res:\n",
        "      print(\">>> No good Google Search Result was found\")\n",
        "      return None\n",
        "  search_results = res[0]\n",
        "  link = search_results['link'] # title, snippet\n",
        "  res = get_page_content(link)\n",
        "  if not res:\n",
        "      print(f\">>> No content was found in {link}\")\n",
        "      return None\n",
        "  retrieved_text = res\n",
        "  trunked_texts = chunk_texts(retrieved_text, 1500)\n",
        "  trunked_texts = [trunked_text.replace('\\n', \" \") for trunked_text in trunked_texts]\n",
        "  return trunked_texts\n"
      ],
      "metadata": {
        "id": "tQl9801fQoew"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_revise_answer(question, answer, content):\n",
        "  revise_prompt = '''\n",
        "I want to revise the answer according to retrieved related text of the question in WIKI pages.\n",
        "You need to check whether the answer is correct.\n",
        "If you find some errors in the answer, revise the answer to make it better.\n",
        "If you find some necessary details are ignored, add it to make the answer more plausible according to the related text.\n",
        "If you find the answer is right and do not need to add more details, just output the original answer directly.\n",
        "**IMPORTANT**\n",
        "Try to keep the structure (multiple paragraphs with its subtitles) in the revised answer and make it more structual for understanding.\n",
        "Split the paragraphs with `\\n\\n` characters.\n",
        "Just output the revised answer directly. DO NOT add additional explanations or annoucement in the revised answer unless you are asked to.\n",
        "'''\n",
        "  revise_model = genai.GenerativeModel('gemini-pro')\n",
        "  messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"parts\": f\"{gemini_system_prompt}\\n##Existing Text in Wiki Web: {content}\\n\\n##Question: {question}\\n\\n##Answer: {answer}\\n\\n##Instruction: {revise_prompt}\"\n",
        "          }\n",
        "      ]\n",
        "  revised_answer = revise_model.generate_content(messages,\n",
        "                                    generation_config=genai.types.GenerationConfig(\n",
        "                                        temperature=1.0,\n",
        "                                        top_k=3,\n",
        "                                        top_p=0.8\n",
        "                                    ))\n",
        "  try:\n",
        "    result = revised_answer.text\n",
        "    return result\n",
        "  except Exception as e:\n",
        "    print(f'{type(e).__name__}: {e}')\n",
        "    return None"
      ],
      "metadata": {
        "id": "v9Zdj1HQQoha"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_query_wrapper(q, question, answer):\n",
        "    result = get_query(question, answer)\n",
        "    q.put(result)\n",
        "\n",
        "def get_content_wrapper(q, query):\n",
        "    result = get_content(query)\n",
        "    q.put(result)\n",
        "\n",
        "def get_revise_answer_wrapper(q, question, answer, content):\n",
        "    result = get_revise_answer(question, answer, content)\n",
        "    q.put(result)\n",
        "\n",
        "from multiprocessing import Process, Queue\n",
        "def run_with_timeout(func, timeout, *args, **kwargs):\n",
        "    q = Queue()  # Create a Queue object for interprocess communication\n",
        "    # Create a process to execute the passed in function, passing the Queue and other *args and **kwargs as parameters\n",
        "    p = Process(target=func, args=(q, *args), kwargs=kwargs)\n",
        "    p.start()\n",
        "    # Wait for process to complete or timeout\n",
        "    p.join(timeout)\n",
        "    if p.is_alive():\n",
        "        print(f\"{datetime.now()} [INFO] The execution of function {str(func)} has timed out ({timeout}s), terminating the process...\")\n",
        "        p.terminate()  # Terminate process\n",
        "        p.join()  # Make sure the process has been terminated\n",
        "        result = None  # In case of timeout, we have no results\n",
        "    else:\n",
        "        print(f\"{datetime.now()} [INFO] Function {str(func)} execution completed successfully\")\n",
        "        result = q.get()  # Get results from queue\n",
        "    return result"
      ],
      "metadata": {
        "id": "dM0h9eEjQoke"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_diff_html(text1, text2):\n",
        "    diff = unified_diff(text1.splitlines(keepends=True),\n",
        "                        text2.splitlines(keepends=True),\n",
        "                        fromfile='text1', tofile='text2')\n",
        "\n",
        "    diff_html = \"\"\n",
        "    for line in diff:\n",
        "        if line.startswith('+'):\n",
        "            diff_html += f\"{line.rstrip()}\"\n",
        "        elif line.startswith('-'):\n",
        "            diff_html += f\"{line.rstrip()}\"\n",
        "        elif line.startswith('@'):\n",
        "            diff_html += f\"{line.rstrip()}\"\n",
        "        else:\n",
        "            diff_html += f\"{line.rstrip()}\"\n",
        "    return diff_html"
      ],
      "metadata": {
        "id": "T0vNtFf_Qom6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RAT Function\n",
        "newline_char = '\\n'\n",
        "\n",
        "def rat(question):\n",
        "    print(f\"{datetime.now()} [INFO] Get draft...\")\n",
        "    draft = get_draft(question)\n",
        "    print(f\"{datetime.now()} [INFO] Return to draft\")\n",
        "    print(f\"##################### DRAFT #######################\")\n",
        "    print(draft)\n",
        "    print(f\"#####################  END  #######################\")\n",
        "\n",
        "    print(f\"{datetime.now()} [INFO] Work on drafts...\")\n",
        "    draft_paragraphs = split_draft(draft)\n",
        "    print(f\"{datetime.now()} [INFO] The draft is divided into {len(draft_paragraphs)} parts\")\n",
        "    answer = \"\"\n",
        "    for i, p in enumerate(draft_paragraphs):\n",
        "        print(str(i)*80)\n",
        "        print(f\"{datetime.now()} [INFO] Modify section {i+1}/{len(draft_paragraphs)}...\")\n",
        "        answer = answer + '\\n\\n' + p\n",
        "        # print(f\"[{i}/{len(draft_paragraphs)}] Original Answer:\\n{answer.replace(newline_char, ' ')}\")\n",
        "\n",
        "        # query = get_query(question, answer)\n",
        "        print(f\"{datetime.now()} [INFO] Generate corresponding Query...\")\n",
        "        res = run_with_timeout(get_query_wrapper, 3, question, answer)\n",
        "        if not res:\n",
        "            print(f\"{datetime.now()} [INFO] Skip next steps...\")\n",
        "            continue\n",
        "        else:\n",
        "            query = res\n",
        "        print(f\">>> {i}/{len(draft_paragraphs)} Query: {query.replace(newline_char, ' ')}\")\n",
        "\n",
        "        print(f\"{datetime.now()} [INFO] Get web content...\")\n",
        "        # content = get_content(query)\n",
        "        res = run_with_timeout(get_content_wrapper, 5, query)\n",
        "        if not res:\n",
        "            print(f\"{datetime.now()} [INFO] Skip next steps...\")\n",
        "            continue\n",
        "        else:\n",
        "            content = res\n",
        "\n",
        "        for j, c in enumerate(content):\n",
        "            if  j > 2:\n",
        "                break\n",
        "            print(f\"{datetime.now()} [INFO] Modify the corresponding answer according to the content of the web page...[{j}/{min(len(content),3)}]\")\n",
        "            # answer = get_revise_answer(question, answer, c)\n",
        "            res = run_with_timeout(get_revise_answer_wrapper, 10, question, answer, c)\n",
        "            if not res:\n",
        "                print(f\"{datetime.now()} [INFO] Skip next steps...\")\n",
        "                continue\n",
        "            else:\n",
        "                diff_html = generate_diff_html(answer, res)\n",
        "                display(HTML(diff_html))\n",
        "                answer = res\n",
        "            print(f\"{datetime.now()} [INFO] Answer modification completed[{j}/{min(len(content),3)}]\")\n",
        "        # print(f\"[{i}/{len(draft_paragraphs)}] REVISED ANSWER:\\n {answer.replace(newline_char, ' ')}\")\n",
        "        # print()\n",
        "    return draft, answer\n",
        "    # return answer\n",
        "\n"
      ],
      "metadata": {
        "id": "9dak1aZ1Qope"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "question = \"How can I get a girlfriend?\"\n",
        "draft, answer = rat(question)"
      ],
      "metadata": {
        "id": "RPWo0emuQor6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "db42bffb-8a0e-4d63-b462-4e6f8058acdf"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-03 06:40:15.917650 [INFO] Get draft...\n",
            "2024-04-03 06:40:21.867570 [INFO] Return to draft\n",
            "##################### DRAFT #######################\n",
            "1. **Be yourself.** This is the most important tip for any aspect of finding a girlfriend. People can tell when you're being fake, so just be yourself and let your personality shine through.\n",
            "2. **Put yourself out there.** You can't meet someone if you're always sitting at home. Join a club, volunteer, or take a class. The more people you meet, the more likely you are to find someone you click with.\n",
            "3. **Be confident.** Confidence is attractive, so work on building your self-esteem. Stand up straight, make eye contact, and smile. People are more likely to be drawn to you if you seem confident and approachable.\n",
            "4. **Be a good listener.** When you're talking to someone, really listen to what they're saying. Ask questions and show that you're interested in what they have to say. People are more likely to want to spend time with you if they feel like you're genuinely interested in them.\n",
            "5. **Be respectful.** Treat others the way you want to be treated. Be polite and respectful, even to people you don't know. People are more likely to want to be around you if they feel like you're a good person.\n",
            "6. **Be patient.** Finding a girlfriend takes time. Don't get discouraged if you don't meet someone right away. Just keep putting yourself out there and eventually you'll find someone special.\n",
            "#####################  END  #######################\n",
            "2024-04-03 06:40:21.867938 [INFO] Work on drafts...\n",
            "2024-04-03 06:40:21.867961 [INFO] The draft is divided into 1 parts\n",
            "00000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
            "2024-04-03 06:40:21.867985 [INFO] Modify section 1/1...\n",
            "2024-04-03 06:40:21.868006 [INFO] Generate corresponding Query...\n",
            "2024-04-03 06:40:23.906897 [INFO] Function <function get_query_wrapper at 0x79a92c4d67a0> execution completed successfully\n",
            ">>> 0/1 Query: How to find a girlfriend patiently\n",
            "2024-04-03 06:40:23.907342 [INFO] Get web content...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching pages:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-03 06:40:28.921739 [INFO] The execution of function <function get_content_wrapper at 0x79a92c4d7b50> has timed out (5s), terminating the process...\n",
            "2024-04-03 06:40:28.929401 [INFO] Skip next steps...\n",
            "CPU times: user 136 ms, sys: 34.7 ms, total: 171 ms\n",
            "Wall time: 13 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "OqS-ZilMsCV2",
        "outputId": "7d76f191-9622-4af5-a045-4bb6562041d1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\n1. **Be yourself.** This is the most important tip for any aspect of finding a girlfriend. People can tell when you're being fake, so just be yourself and let your personality shine through.\\n2. **Put yourself out there.** You can't meet someone if you're always sitting at home. Join a club, volunteer, or take a class. The more people you meet, the more likely you are to find someone you click with.\\n3. **Be confident.** Confidence is attractive, so work on building your self-esteem. Stand up straight, make eye contact, and smile. People are more likely to be drawn to you if you seem confident and approachable.\\n4. **Be a good listener.** When you're talking to someone, really listen to what they're saying. Ask questions and show that you're interested in what they have to say. People are more likely to want to spend time with you if they feel like you're genuinely interested in them.\\n5. **Be respectful.** Treat others the way you want to be treated. Be polite and respectful, even to people you don't know. People are more likely to want to be around you if they feel like you're a good person.\\n6. **Be patient.** Finding a girlfriend takes time. Don't get discouraged if you don't meet someone right away. Just keep putting yourself out there and eventually you'll find someone special.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    }
  ]
}