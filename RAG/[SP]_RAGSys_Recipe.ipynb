{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLk7xm2lA95Q"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYWqjnVUA9Ir",
        "outputId": "0900af9b-be05-42b3-e974-6fe21d4ade61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.1.5-py3-none-any.whl (806 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/806.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/806.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.7/806.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai\n",
            "  Downloading langchain_openai-0.0.5-py3-none-any.whl (29 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.17 (from langchain)\n",
            "  Downloading langchain_community-0.0.17-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.16 (from langchain)\n",
            "  Downloading langchain_core-0.1.18-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.0/237.0 kB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1,>=0.0.83 (from langchain)\n",
            "  Downloading langsmith-0.0.85-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Collecting openai<2.0.0,>=1.10.0 (from langchain-openai)\n",
            "  Downloading openai-1.10.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.1/225.1 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.66.1)\n",
            "Collecting typing-extensions<5,>=4.7 (from openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.16->langchain) (1.2.0)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: faiss-cpu, typing-extensions, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, tiktoken, jsonpatch, httpcore, langsmith, httpx, dataclasses-json, openai, langchain-core, langchain-openai, langchain-community, langchain\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.4 faiss-cpu-1.7.4 h11-0.14.0 httpcore-1.0.2 httpx-0.26.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.5 langchain-community-0.0.17 langchain-core-0.1.18 langchain-openai-0.0.5 langsmith-0.0.85 marshmallow-3.20.2 mypy-extensions-1.0.0 openai-1.10.0 tiktoken-0.5.2 typing-extensions-4.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade langchain langchain-openai faiss-cpu tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ItITsvv9A9Ny"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"...\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yT7IVJFBB3R"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82YZ2bT8BEDT"
      },
      "source": [
        "## Naive RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wq-EbD8sA9Qf"
      },
      "outputs": [],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UuorCLBPA9TP"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-06-07-exploration-drl/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-08-06-nas/\",\n",
        "           \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot\"\n",
        "           ),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter()\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = ChatOpenAI()\n",
        "\n",
        "retrieval_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "0GuDvbmZBioW",
        "outputId": "37368e13-81a2-4b3d-9016-2f86b136a5e5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Augmented Language Models are language models that have been enhanced with reasoning skills and the ability to use external tools. They are discussed in a survey by Mialon et al. in 2023.'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"What is the Augmented Language Models\"\n",
        "retrieval_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyIh5FRLBFzB"
      },
      "source": [
        "## Advanced RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ49xC608RhB"
      },
      "source": [
        "### Re-ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEm-1WyjA9Y9",
        "outputId": "31cded55-8065-43c8-a1b6-22e2340af2c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-4.45-py3-none-any.whl (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (3.9.3)\n",
            "Collecting backoff<3.0,>=2.0 (from cohere)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting fastavro<2.0,>=1.8 (from cohere)\n",
            "  Downloading fastavro-1.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib_metadata<7.0,>=6.0 (from cohere)\n",
            "  Downloading importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.31.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.0.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib_metadata<7.0,>=6.0->cohere) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.25.0->cohere) (2023.11.17)\n",
            "Installing collected packages: importlib_metadata, fastavro, backoff, cohere\n",
            "  Attempting uninstall: importlib_metadata\n",
            "    Found existing installation: importlib-metadata 7.0.1\n",
            "    Uninstalling importlib-metadata-7.0.1:\n",
            "      Successfully uninstalled importlib-metadata-7.0.1\n",
            "Successfully installed backoff-2.2.1 cohere-4.45 fastavro-1.9.3 importlib_metadata-6.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sq6v0Oyb9azz"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EG6tCEn-Aak",
        "outputId": "23f49679-b982-4251-997a-42a4beb609fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "COHERE_API_KEY··········\n"
          ]
        }
      ],
      "source": [
        "COHERE_API_KEY = \"...\"\n",
        "os.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Wos7UAvb4-wB"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-06-07-exploration-drl/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-08-06-nas/\",\n",
        "           \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot\"\n",
        "           ),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Dk8nv2RH4-yO"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                      chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "39quJmYt4-0s"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tw9K3KMyA9bc"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = ChatOpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4UrdQn809tsm"
      },
      "outputs": [],
      "source": [
        "llm = Cohere(temperature=0)\n",
        "compressor = CohereRerank()\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uVqRL1CwA9eL"
      },
      "outputs": [],
      "source": [
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "retrieval_chain = (\n",
        "    {\"context\": compression_retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "jFeORgCLA9hD",
        "outputId": "8d0fb78f-ba3f-4b20-8294-06f4b473e4f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Augmented Language Models (ALMs) are language models that have been enhanced with reasoning skills and the capacity to utilize external tools. According to a survey by Mialon et al. in 2023, they cover a wide range of categories of language models that have been augmented. One example of an ALM is Toolformer, a model that has learned to use tools. Another example would be PAL (Program-Aided Language models.) These models utilize external knowledge bases to retrieve content for response generation. Generally, there are many different approaches to augmenting language models. '"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"What is the Augmented Language Models\"\n",
        "retrieval_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqhUgyqZHbSE"
      },
      "source": [
        "### HYDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ggXK84tDFIxh"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import HypotheticalDocumentEmbedder, LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "0p-592PgFIzt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "  Re-implemented from: https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb\n",
        "\"\"\"\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-06-07-exploration-drl/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-08-06-nas/\",\n",
        "           \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot\"\n",
        "           ),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                      chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "XPTzMUjxFI2U"
      },
      "outputs": [],
      "source": [
        "base_embeddings = OpenAIEmbeddings()\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "# Load with `web_search` prompt\n",
        "embeddings = HypotheticalDocumentEmbedder.from_llm(llm, base_embeddings, \"web_search\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6ByzhS8dFI46"
      },
      "outputs": [],
      "source": [
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "BCMSH-EbFI7W"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "model = ChatOpenAI()\n",
        "\n",
        "\n",
        "retrieval_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "XjLHj7MoFI96",
        "outputId": "7997b1ed-c4dc-46e8-e377-1eba1851bb5a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Augmented Language Models refer to language models that have been enhanced with reasoning skills and the ability to use external tools. They are designed to go beyond traditional language models by incorporating additional capabilities for tasks that require the latest knowledge or access to external knowledge bases.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"What is the Augmented Language Models\"\n",
        "retrieval_chain.invoke(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbSzy23rL-P6"
      },
      "source": [
        "### Meta-Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "CyHCZ2MCFJAf"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "c_aCfBoAFJC-"
      },
      "outputs": [],
      "source": [
        "def initialize_chain(instructions, memory=None):\n",
        "    if memory is None:\n",
        "        memory = ConversationBufferWindowMemory()\n",
        "        memory.ai_prefix = \"Assistant\"\n",
        "\n",
        "    template = f\"\"\"\n",
        "    Instructions: {instructions}\n",
        "    {{{memory.memory_key}}}\n",
        "    Human: {{human_input}}\n",
        "    Assistant:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"history\", \"human_input\"], template=template\n",
        "    )\n",
        "\n",
        "    chain = LLMChain(\n",
        "        llm=OpenAI(temperature=0),\n",
        "        prompt=prompt,\n",
        "        verbose=True,\n",
        "        memory=ConversationBufferWindowMemory(),\n",
        "    )\n",
        "    return chain\n",
        "\n",
        "\n",
        "def initialize_meta_chain():\n",
        "    meta_template = \"\"\"\n",
        "    Assistant has just had the below interactions with a User. Assistant followed their \"Instructions\" closely. Your job is to critique the Assistant's performance and then revise the Instructions so that Assistant would quickly and correctly respond in the future.\n",
        "\n",
        "    ####\n",
        "\n",
        "    {chat_history}\n",
        "\n",
        "    ####\n",
        "\n",
        "    Please reflect on these interactions.\n",
        "\n",
        "    You should first critique Assistant's performance. What could Assistant have done better? What should the Assistant remember about this user? Are there things this user always wants? Indicate this with \"Critique: ...\".\n",
        "\n",
        "    You should next revise the Instructions so that Assistant would quickly and correctly respond in the future. Assistant's goal is to satisfy the user in as few interactions as possible. Assistant will only see the new Instructions, not the interaction history, so anything important must be summarized in the Instructions. Don't forget any important details in the current Instructions! Indicate the new Instructions by \"Instructions: ...\".\n",
        "    \"\"\"\n",
        "\n",
        "    meta_prompt = PromptTemplate(\n",
        "        input_variables=[\"chat_history\"], template=meta_template\n",
        "    )\n",
        "\n",
        "    meta_chain = LLMChain(\n",
        "        llm=OpenAI(temperature=0),\n",
        "        prompt=meta_prompt,\n",
        "        verbose=True,\n",
        "    )\n",
        "    return meta_chain\n",
        "\n",
        "\n",
        "def get_chat_history(chain_memory):\n",
        "    memory_key = chain_memory.memory_key\n",
        "    chat_history = chain_memory.load_memory_variables(memory_key)[memory_key]\n",
        "    return chat_history\n",
        "\n",
        "\n",
        "def get_new_instructions(meta_output):\n",
        "    delimiter = \"Instructions: \"\n",
        "    new_instructions = meta_output[meta_output.find(delimiter) + len(delimiter) :]\n",
        "    return new_instructions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "AcNcFanwMHr6"
      },
      "outputs": [],
      "source": [
        "def main(task, max_iters=3, max_meta_iters=5):\n",
        "    failed_phrase = \"task failed\"\n",
        "    success_phrase = \"task succeeded\"\n",
        "    key_phrases = [success_phrase, failed_phrase]\n",
        "\n",
        "    instructions = \"None\"\n",
        "    for i in range(max_meta_iters):\n",
        "        print(f\"[Episode {i+1}/{max_meta_iters}]\")\n",
        "        chain = initialize_chain(instructions, memory=None)\n",
        "        output = chain.predict(human_input=task)\n",
        "        for j in range(max_iters):\n",
        "            print(f\"(Step {j+1}/{max_iters})\")\n",
        "            print(f\"Assistant: {output}\")\n",
        "            print(\"Human: \")\n",
        "            human_input = input()\n",
        "            if any(phrase in human_input.lower() for phrase in key_phrases):\n",
        "                break\n",
        "            output = chain.predict(human_input=human_input)\n",
        "        if success_phrase in human_input.lower():\n",
        "            print(\"You succeeded! Thanks for playing!\")\n",
        "            return\n",
        "        meta_chain = initialize_meta_chain()\n",
        "        meta_output = meta_chain.predict(chat_history=get_chat_history(chain.memory))\n",
        "        print(f\"Feedback: {meta_output}\")\n",
        "        instructions = get_new_instructions(meta_output)\n",
        "        print(f\"New Instructions: {instructions}\")\n",
        "        print(\"\\n\" + \"#\" * 80 + \"\\n\")\n",
        "    print(\"You failed! Thanks for playing!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaaqcAYGMQCS"
      },
      "outputs": [],
      "source": [
        "task = \"Provide a systematic argument for why we should always eat pasta with olives.\"\n",
        "main(task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvxzqDOpNOIn"
      },
      "source": [
        "### Fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "2RG3_pTgOzvF"
      },
      "outputs": [],
      "source": [
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2020-01-29-curriculum-rl/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/\",\n",
        "           \"https://lilianweng.github.io/posts/2020-06-07-exploration-drl/\",\n",
        "           \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/#chain-of-thought-cot\"\n",
        "           ),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                      chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sfX4YiylNPPZ"
      },
      "outputs": [],
      "source": [
        "from langchain.load import dumps, loads\n",
        "\"\"\"\n",
        "Re implemented from: https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb\n",
        "\"\"\"\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        # Assumes the docs are returned in sorted order of relevance\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "    return reranked_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "HhUQw-NqNPXC"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "retrieval_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "fusion = retrieval_chain | retriever.map() | reciprocal_rank_fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nSvwRu2NPZ3"
      },
      "outputs": [],
      "source": [
        "question = \"What is the Augmented Language Models\"\n",
        "fusion.invoke(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiAUPW2ZOuyb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
